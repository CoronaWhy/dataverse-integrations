{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation is mostly taken from https://github.com/AUSSDA/pyDataverse_demo_tromso/blob/master/pydataverse.ipynb\n",
    "\n",
    "Atuhor: Matthieu Pons (pons.matthieu@gmail.com or https://github.com/mpons for support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Python modules\n",
    "import json\n",
    "import os\n",
    "import subprocess as sp\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from pyDataverse.api import Api, NativeApi\n",
    "from pyDataverse.models import Datafile, Dataset\n",
    "from pyDataverse.utils import read_file_csv_to_dict\n",
    "from pyDataverse.utils import read_file\n",
    "\n",
    "from config import LOCAL_RESSOURCES_FOLDER, DV_ALIAS, BASE_URL, API_TOKEN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dataset_keys(dataset_row, data, terms_filename):\n",
    "    if pd.isnull(dataset_row['organization.dataset_id']):\n",
    "        return data\n",
    "    \n",
    "    ds_tmp = {}\n",
    "    ds_id = dataset_row['organization.dataset_id']\n",
    "    ds_tmp['termsOfAccess'] = read_file(terms_filename)\n",
    "    \n",
    "    if not pd.isnull(dataset_row['dataverse.title']):\n",
    "        ds_tmp['title'] = dataset_row['dataverse.title']\n",
    "    \n",
    "    if not pd.isnull(dataset_row['dataverse.subtitle']):\n",
    "        ds_tmp['subtitle'] = dataset_row['dataverse.subtitle']\n",
    "    \n",
    "    if not pd.isnull(dataset_row['dataverse.author']):\n",
    "        ds_tmp['author'] = json.loads(dataset_row['dataverse.author'])\n",
    "    \n",
    "    if not pd.isnull(dataset_row['dataverse.dsDescription']):\n",
    "        ds_tmp['dsDescription'] = [{'dsDescriptionValue': dataset_row['dataverse.dsDescription']}]\n",
    "    \n",
    "    if not pd.isnull(dataset_row['dataverse.keywordValue']):\n",
    "        ds_tmp['keyword'] = json.loads(dataset_row['dataverse.keywordValue'])\n",
    "    \n",
    "    if not pd.isnull(dataset_row['dataverse.topicClassification']):\n",
    "        ds_tmp['topicClassification'] = json.loads(dataset_row['dataverse.topicClassification'])\n",
    "    \n",
    "    if not pd.isnull(dataset_row['dataverse.language']):\n",
    "        ds_tmp['language'] = json.loads(dataset_row['dataverse.language'])\n",
    "    \n",
    "    if not pd.isnull(dataset_row['dataverse.subject']):\n",
    "        ds_tmp['subject'] = [dataset_row['dataverse.subject']]\n",
    "    \n",
    "    if not pd.isnull(dataset_row['dataverse.kindOfData']):\n",
    "        ds_tmp['kindOfData'] = json.loads(dataset_row['dataverse.kindOfData'])\n",
    "    \n",
    "    if not pd.isnull(dataset_row['dataverse.datasetContact']):\n",
    "        ds_tmp['datasetContact'] = json.loads(dataset_row['dataverse.datasetContact'])\n",
    "    \n",
    "    data[ds_id] = {'metadata': ds_tmp}\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_datafile(datafile_row, data):\n",
    "    df_tmp = {}\n",
    "    df_id = None\n",
    "    ds_id = None\n",
    "    if not pd.isnull(datafile_row['dataverse.description']):\n",
    "        df_tmp['description'] = datafile_row['dataverse.description']\n",
    "        \n",
    "    if not pd.isnull(datafile_row['organization.filename']):\n",
    "        df_tmp['filename'] = datafile_row['organization.filename']\n",
    "    if not pd.isnull(datafile_row['organization.datafile_id']):\n",
    "        df_tmp['datafile_id'] = datafile_row['organization.datafile_id']\n",
    "        df_id = datafile_row['organization.datafile_id']\n",
    "    if not pd.isnull(datafile_row['organization.dataset_id']):\n",
    "        ds_id = datafile_row['organization.dataset_id']\n",
    "        df_tmp['dataset_id'] = ds_id\n",
    "    if not pd.isnull(datafile_row['dataverse.categories']):\n",
    "        df_tmp['categories'] = json.loads(datafile_row['dataverse.categories'])\n",
    "        \n",
    "    if 'datafiles' not in data[ds_id]:\n",
    "        data[ds_id]['datafiles'] = {}\n",
    "    if df_id not in data[ds_id]['datafiles']:\n",
    "        data[ds_id]['datafiles'][df_id] = {}\n",
    "    if 'metadata' not in data[ds_id]['datafiles'][df_id]:\n",
    "        data[ds_id]['datafiles'][df_id]['metadata'] = {}\n",
    "    data[ds_id]['datafiles'][df_id]['metadata'] = df_tmp\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(api, ds, dv_alias, mapping_dsid2pid, ds_id, base_url):\n",
    "    try:\n",
    "        resp = api.create_dataset(dv_alias, ds.json())\n",
    "        pid = resp.json()['data']['persistentId']\n",
    "    except:\n",
    "        print(resp.content)\n",
    "        return resp, mapping_dsid2pid\n",
    "    \n",
    "    mapping_dsid2pid[ds_id] = pid\n",
    "    time.sleep(1)\n",
    "    print('{0}/dataset.xhtml?persistentId={1}&version=DRAFT'.format(base_url,\n",
    "                                                                    pid))\n",
    "    return resp, mapping_dsid2pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_datafile(api, pid, filename, df):\n",
    "    path = api.base_url\n",
    "    path += '/datasets/:persistentId/add?persistentId={0}'.format(pid)\n",
    "    shell_command = 'curl -H \"X-Dataverse-key: {0}\"'.format(api.api_token)\n",
    "    shell_command += ' -X POST {0} -F file=@{1}'.format(path, filename)\n",
    "    shell_command += \" -F 'jsonData={0}'\".format(df.json())\n",
    "    result = sp.run(shell_command, shell=True, stdout=sp.PIPE)\n",
    "    if filename[-4:] == '.sav' or filename[-4:] == '.dta':\n",
    "        time.sleep(20)\n",
    "    else:\n",
    "        time.sleep(2)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_dataset(pid, api):\n",
    "    resp = api.delete_dataset(pid)\n",
    "    time.sleep(1)\n",
    "    return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def publish_dataset(pid, api):\n",
    "    resp = api.publish_dataset(pid, 'major')\n",
    "    print(resp.json())\n",
    "    return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_filename = os.path.join(LOCAL_RESSOURCES_FOLDER, 'datasets.csv')\n",
    "license_filename = os.path.join(LOCAL_RESSOURCES_FOLDER, 'license.html')\n",
    "terms_filename = os.path.join(LOCAL_RESSOURCES_FOLDER, 'terms-of-access.html')\n",
    "\n",
    "data = {}\n",
    "license_default = read_file(license_filename)\n",
    "datasets_csv = read_file_csv_to_dict(ds_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Datasets metadata from CSV file and save it in a dictionary\n",
    "datasets_df = pd.read_csv(ds_filename)\n",
    "data = {}\n",
    "for dataset_row in datasets_df.iterrows():\n",
    "    data = parse_dataset_keys(dataset_row[1], data, terms_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "native_api = NativeApi(BASE_URL, API_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset with pid 'doi:10.5072/FK2/M92KZV' created.\n",
      "http://datasets.coronawhy.org/dataset.xhtml?persistentId=doi:10.5072/FK2/M92KZV&version=DRAFT\n",
      "Dataset with pid 'doi:10.5072/FK2/SIQOBX' created.\n",
      "http://datasets.coronawhy.org/dataset.xhtml?persistentId=doi:10.5072/FK2/SIQOBX&version=DRAFT\n"
     ]
    }
   ],
   "source": [
    "mapping_dsid2pid = {}\n",
    "\n",
    "for ds_id, dataset in data.items():\n",
    "    ds = Dataset()\n",
    "    ds.set(dataset['metadata'])\n",
    "    \n",
    "    ds.displayName=dataset['metadata']['title']\n",
    "    \n",
    "    resp, mapping_dsid2pid = create_dataset(native_api, ds, DV_ALIAS, mapping_dsid2pid, ds_id, BASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filename = os.path.join(LOCAL_RESSOURCES_FOLDER, 'datafiles.csv')\n",
    "datafiles_df = pd.read_csv(df_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for datafile_row in datafiles_df.iterrows():\n",
    "    data = import_datafile(datafile_row[1], data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload Datafile metadata and data via API\n",
    "\n",
    "for ds_id, dataset in data.items():\n",
    "    pid = mapping_dsid2pid[ds_id]\n",
    "    for df_id, datafile in dataset['datafiles'].items():\n",
    "        data_tmp = datafile['metadata']\n",
    "        data_tmp['pid'] = pid\n",
    "        df = Datafile()\n",
    "        df.set(data_tmp)\n",
    "        filename = os.path.abspath(os.path.join('dataverse', 'files', datafile['metadata']['filename']))\n",
    "        resp = upload_datafile(native_api, pid, filename, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'doi:10.5072/FK2/M92KZV' deleted.\n",
      "Dataset 'doi:10.5072/FK2/SIQOBX' deleted.\n"
     ]
    }
   ],
   "source": [
    "# Delete the Datasets at the End (OPTIONAL)\n",
    "DELETE_DATASETS = DV_ALIAS == 'demo'\n",
    "\n",
    "if DELETE_DATASETS:\n",
    "    for ds_id, dataset in data.items():\n",
    "        resp = delete_dataset(mapping_dsid2pid[ds_id], native_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
